DataOps LocalStack Pipeline
ğŸ“Œ Objective
Build a fully automated and reproducible DataOps pipeline using modern DevOps and data engineering tools.
The pipeline provisions infrastructure using Terraform, runs a Dockerized ETL process, and is fully automated using GitHub Actions CI/CD, with all AWS services emulated locally using LocalStack.
This project demonstrates Infrastructure as Code, containerization, orchestration, and CI/CD automation in a safe, cost-free local environment.
ğŸ—ï¸ Architecture Overview
Terraform â€“ Provision AWS S3 bucket (via LocalStack)
LocalStack â€“ Local AWS cloud emulation
Python ETL â€“ Extract, Transform, Load CSV data
Docker â€“ Containerize ETL application
Docker Compose â€“ Local orchestration
GitHub Actions â€“ CI/CD automation
ğŸ“ Repository Structure
dataops-localstack-pipeline/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ docker-compose.yml
â”‚
â”œâ”€â”€ terraform/
â”‚   â”œâ”€â”€ main.tf
â”‚   â”œâ”€â”€ variables.tf
â”‚   â””â”€â”€ outputs.tf
â”‚
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ etl.py
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”‚
â””â”€â”€ .github/
    â””â”€â”€ workflows/
        â””â”€â”€ ci-cd-pipeline.yml
Implementation Steps
âœ… Step 1: Project Setup & Git Initialization
Create project folder
Initialize Git repository
Add README.md
Push initial structure to GitHub
Copy code
Powershell
git init
git add README.md
git commit -m "Step 1: Initial project setup"
git push origin main
âœ… Step 2: Infrastructure as Code (Terraform + LocalStack)
Terraform configured for LocalStack
Creates S3 bucket using dummy AWS credentials
Uses variables for bucket name and region
Key files:
terraform/main.tf
terraform/variables.tf
terraform/outputs.tf
Commands:
Copy code
Powershell
terraform init
terraform validate
terraform plan
terraform apply -auto-approve
âœ… Step 3: ETL Application (Python + Docker)
Python ETL script:
Reads CSV from raw/
Transforms data (filter + computed column)
Writes CSV to processed/
Dockerized using a clean Dockerfile
Files:
etl/etl.py
etl/requirements.txt
etl/Dockerfile
âœ… Step 4: Local Development (Docker Compose)
Orchestrates:
LocalStack container
ETL container
Enables full local testing before CI
Command:
Copy code
Powershell
docker-compose up --build
âœ… Step 5: CI/CD Automation (GitHub Actions)
Triggered on every push to main
Automates:
Start LocalStack
Terraform init / apply
Build & run ETL container
Verify output file in S3
Workflow file:
Copy code

.github/workflows/ci-cd-pipeline.yml
âœ… Step 6: Cleanup Old Workflow
Removed unused ci.yml workflow
Ensured only the correct CI/CD workflow remains
Copy code
Powershell
git rm .github/workflows/ci.yml
git commit -m "Step 6: Remove old workflow"
git push origin main
âœ… Step 7: Local Verification
Verified locally that:
Terraform provisions S3 in LocalStack
LocalStack container runs
ETL container executes successfully
Output file appears in processed/
Copy code
Powershell
aws --endpoint-url=http://localhost:4566 s3 ls s3://dataops-local-bucket/processed/
âœ… Step 8: CI/CD Verification
Triggered GitHub Actions by pushing a commit
Verified successful workflow execution in Actions tab
Confirmed ETL output file existence via CI job
âœ… Step 9: Final Submission Readiness
All files committed and pushed
Local execution verified
CI/CD pipeline verified
Project fully reproducible and automated
ğŸ¯ Expected Outcome
Fully automated DataOps pipeline
No manual AWS account required
Safe local AWS emulation
Industry-standard DevOps practices
CI/CD-ready data engineering workflow
ğŸ§ª Technologies Used
Terraform
Docker & Docker Compose
Python (Pandas, Boto3)
LocalStack
GitHub Actions
AWS CLI
ğŸ“Œ Proof of Execution
âœ” Successful GitHub Actions workflow runs are visible in the Actions tab
âœ” processed/output.csv is generated by the ETL job
ğŸ Conclusion
This project demonstrates a professional-grade DataOps pipeline, bridging data engineering and DevOps using modern tooling and automation best practices.